{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ce34938-2fbf-4cb9-956e-1a820da81969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved updated train -> C:\\Users\\AYUSH SINGH\\Documents\\GitHub\\NervSightX\\Machine learning\\dreaddit_cv_raw_splits\\train_raw_with_clean_text.csv\n",
      "Saved updated test  -> C:\\Users\\AYUSH SINGH\\Documents\\GitHub\\NervSightX\\Machine learning\\dreaddit_cv_raw_splits\\test_frozen_raw_with_clean_text.csv\n",
      "\n",
      "train_merged shape: (572, 114)\n",
      "test_merged shape:  (143, 114)\n",
      "\n",
      "label distribution (train):\n",
      "label\n",
      "1    295\n",
      "0    277\n",
      "\n",
      "label distribution (test):\n",
      "label\n",
      "1    74\n",
      "0    69\n",
      "\n",
      "Sample clean_text (train):\n",
      " orig_index  label                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    clean_text\n",
      "        136      1                                                                                                                                                                                                                                             but i have been on about a dozen times and have been hung up on nine it is always the same issue sadly i get depressed it is late i cannot sleep or stop thinking about past trauma and i would just like to talk to someone but the bulk of them have hung up some almost immediately and some after longer it is the longer ones that hurt more\n",
      "        144      0                                                                                                                                                                                                                                                                                     hey redditors i am looking for advice and suggestions about what i should do with my current living situation i have lived in my current apartment for 2 years and took up tenancy after i separated from my ex husband he kept our apartment the apartment i m living in is comfortable enough it s mine\n",
      "        414      1                                                                                                                                                                                                                                             plus she sent him pictures of her ultrasound now i m just heart broken because i was head over heels for this guy and i m also disgusted by what he did to this girl i don t know how to confront him about this or how to handle this situation properly i am going to break up with him but what do i do after i end it? do i tell his fiancee?\n",
      "        453      0 hi guys i m a current english major at my first year in uni trying to save up enough money to participate in an in depth language study in hokkaido japan my career goal has always been to teach english in asia and after years of deliberation and comparing the different countries i could teach in i believe japan is best suited to me the only problem is my current school only offers japanese 1 and 2 and i need to be as close to fluent as possible in order to give me an edge in the hiring process i know the prospect of studying abroad in asia seems like a cushy vacation\n",
      "          5      0                                                                                                                                                                                                                                                                                                 thanks edit 1 fuel receipt as requested url sorry for the long responses i went to spend the night at a friends because it got really cold here! the police said they don t give out a copy of the report but they gave me an incident number that can be used to verify the report was filed\n",
      "\n",
      "Sample clean_text (test):\n",
      " orig_index  label                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   clean_text\n",
      "        685      0                                                                                                                                                                       what are you gonna do with that strange looking super shiny thing over there? that s not really going anywhere near me is it??? ? wish i could go on my phone right now and play a silly game to distract myself but i ve just got to sit here while dr fiddles around in my mouth i love the way the drill makes a sound like kittens\n",
      "        532      1         but what should i say? part of me wants to tell her i m sorry for being a shit boyfriend at the start and that as i ve gotten older i ve gotten wiser and more experience but would she even care about that shit? or should i just start off with hey haven t seen you in awhile hows everything and start from there? ugh maybe i m overthinking this anyways would really appreciate you guys or gals out there with more experience than me guiding me through this minor plight i put myself in\n",
      "        268      1 hey guys i have ptsd from years of emotional abuse and neglect by my narcissistic parents i am living in their house again now due to some financial hardships im 22 i am constantly being triggered and am experiencing intensified symptoms does anyone have any tips on how to deal with this situation? i feel completely trapped i need to figure out how to manage these symptoms because they re really getting in the way of my functioning sorry for the lack of detail emotion but i am so drained\n",
      "        507      1                                                                                                                                                                                                                                                                             we had 2 classes together so we spent a few hours together most days working through problem sets this next semester i won t even have that i ll probably be in more isolation this time around any tips are appreciated thanks!\n",
      "        465      1                                                               it s really just standard issue big corporation stuff my direct manager slaps together the scheduling for our dept at the last minute because she s overworked and she s overworked because it s cheaper to make her do enough work for 2 3 people than to just open up an assistant position i m not getting enough sleep i m going off my medication because i can t make the appointments i need my weekly schedule is a complete dice roll\n"
     ]
    }
   ],
   "source": [
    "# Fixed Cell — create clean_text and merge into saved train/test splits\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = Path(\"dreaddit_StressAnalysis - Sheet1.csv\")\n",
    "OUT_DIR = Path(\"dreaddit_cv_raw_splits\")\n",
    "TRAIN_PATH = OUT_DIR / \"train_raw.csv\"\n",
    "TEST_PATH  = OUT_DIR / \"test_frozen_raw.csv\"\n",
    "\n",
    "def clean_text_func(t):\n",
    "    if pd.isna(t):\n",
    "        return \"\"\n",
    "    text = str(t).lower()\n",
    "\n",
    "    # remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)\n",
    "\n",
    "    # remove emails\n",
    "    text = re.sub(r'\\S+@\\S+\\.\\S+', ' ', text)\n",
    "\n",
    "    # remove markdown links [text](url)\n",
    "    text = re.sub(r'\\[.*?\\]\\(.*?\\)', ' ', text)\n",
    "\n",
    "    # remove user mentions @name or u/name\n",
    "    text = re.sub(r'@[A-Za-z0-9_]+', ' ', text)\n",
    "    text = re.sub(r'u/[A-Za-z0-9_]+', ' ', text)\n",
    "\n",
    "    # remove fenced code blocks ```...```\n",
    "    text = re.sub(r'```.*?```', ' ', text, flags=re.DOTALL)\n",
    "\n",
    "    # remove inline code `...`\n",
    "    text = re.sub(r'`[^`]*`', ' ', text)\n",
    "\n",
    "    # remove runs of asterisks/underscores (markdown emphasis) safely\n",
    "    text = re.sub(r'[\\*_]{1,}', ' ', text)\n",
    "\n",
    "    # remove blockquote lines starting with >\n",
    "    text = re.sub(r'(^|\\n)>\\s*.*', ' ', text)\n",
    "\n",
    "    # keep letters, numbers, space, ? and !\n",
    "    text = re.sub(r'[^a-z0-9?! ]+', ' ', text)\n",
    "\n",
    "    # normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Load original CSV and create clean_text\n",
    "orig = pd.read_csv(DATA_PATH)\n",
    "orig['clean_text'] = orig['text'].apply(clean_text_func)\n",
    "\n",
    "# Load saved splits\n",
    "if not TRAIN_PATH.exists() or not TEST_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Expected saved splits at {TRAIN_PATH} and {TEST_PATH}. Run earlier split cells first.\")\n",
    "\n",
    "train_raw = pd.read_csv(TRAIN_PATH)\n",
    "test_raw  = pd.read_csv(TEST_PATH)\n",
    "\n",
    "# Merge clean_text into train/test using orig_index\n",
    "if 'orig_index' not in train_raw.columns or 'orig_index' not in test_raw.columns:\n",
    "    raise KeyError(\"train_raw.csv or test_frozen_raw.csv missing 'orig_index' column.\")\n",
    "\n",
    "train_merged = train_raw.merge(orig[['clean_text']], left_on='orig_index', right_index=True, how='left')\n",
    "test_merged  = test_raw.merge(orig[['clean_text']],  left_on='orig_index', right_index=True, how='left')\n",
    "\n",
    "# Save updated files\n",
    "train_out_path = OUT_DIR / \"train_raw_with_clean_text.csv\"\n",
    "test_out_path  = OUT_DIR / \"test_frozen_raw_with_clean_text.csv\"\n",
    "\n",
    "train_merged.to_csv(train_out_path, index=False)\n",
    "test_merged.to_csv(test_out_path, index=False)\n",
    "\n",
    "# Print results\n",
    "print(\"Saved updated train ->\", train_out_path.resolve())\n",
    "print(\"Saved updated test  ->\", test_out_path.resolve())\n",
    "print()\n",
    "print(\"train_merged shape:\", train_merged.shape)\n",
    "print(\"test_merged shape: \", test_merged.shape)\n",
    "print()\n",
    "print(\"label distribution (train):\")\n",
    "print(train_merged['label'].value_counts().to_string())\n",
    "print()\n",
    "print(\"label distribution (test):\")\n",
    "print(test_merged['label'].value_counts().to_string())\n",
    "print()\n",
    "print(\"Sample clean_text (train):\")\n",
    "print(train_merged[['orig_index','label','clean_text']].head(5).to_string(index=False))\n",
    "print()\n",
    "print(\"Sample clean_text (test):\")\n",
    "print(test_merged[['orig_index','label','clean_text']].head(5).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a70aa04c-7319-42b1-863d-2247a9d1f114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train/test with clean_text...\n",
      "Train shape: (572, 114)\n",
      "Test  shape: (143, 114)\n",
      "Fitting TF-IDF on train clean_text ...\n",
      "Fitted. Vocabulary size: 2051\n",
      "\n",
      "Saved vectorizer -> C:\\Users\\AYUSH SINGH\\Documents\\GitHub\\NervSightX\\Machine learning\\dreaddit_cv_raw_splits\\tfidf\\tfidf_vectorizer.joblib\n",
      "Saved X_train_tfidf -> C:\\Users\\AYUSH SINGH\\Documents\\GitHub\\NervSightX\\Machine learning\\dreaddit_cv_raw_splits\\tfidf\\X_train_tfidf.npz\n",
      "Saved X_test_tfidf  -> C:\\Users\\AYUSH SINGH\\Documents\\GitHub\\NervSightX\\Machine learning\\dreaddit_cv_raw_splits\\tfidf\\X_test_tfidf.npz\n",
      "\n",
      "X_train_tfidf shape: (572, 2051), nnz: 37944, density: 0.032343\n",
      "\n",
      "X_test_tfidf shape: (143, 2051), nnz: 8627, density: 0.029414\n",
      "\n",
      "Top 20 most common n-grams (lowest idf):\n",
      "['and', 'the', 'my', 'of', 'it', 'in', 'that', 'me', 'for', 'but', 'with', 'is', 'this', 'have', 'was', 'on', 'so', 'not', 'like', 'or']\n",
      "\n",
      "Top 20 rarest n-grams (highest idf):\n",
      "['x200b', 'writing', 'would love', 'afraid to', 'advice on', 'advance', 'pass', 'over to', 'personality', 'people in', 'peace', 'pay rent', 'past few', 'past and', 'doctors', 'do that', 'help them', 'have some', 'him he', 'higher']\n",
      "\n",
      "Sample train nonzero counts (first 6 rows): [54, 47, 68, 71, 51, 53]\n",
      "Sample test nonzero counts  (first 6 rows): [55, 64, 74, 35, 45, 67]\n"
     ]
    }
   ],
   "source": [
    "# Cell — TF-IDF: fit on train_clean_text, transform train & frozen test, save artifacts\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scipy.sparse as sp\n",
    "\n",
    "OUT_DIR = Path(\"dreaddit_cv_raw_splits\")\n",
    "TRAIN_IN = OUT_DIR / \"train_raw_with_clean_text.csv\"\n",
    "TEST_IN  = OUT_DIR / \"test_frozen_raw_with_clean_text.csv\"\n",
    "VECT_DIR = OUT_DIR / \"tfidf\"\n",
    "VECT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# TF-IDF hyperparams (edit before running if you want)\n",
    "ngram_range = (1,2)\n",
    "min_df = 5           # min doc freq (int) or float (proportion)\n",
    "max_df = 0.9         # max doc freq (float proportion) or int\n",
    "max_features = 50000 # None or int\n",
    "\n",
    "print(\"Loading train/test with clean_text...\")\n",
    "train_df = pd.read_csv(TRAIN_IN)\n",
    "test_df  = pd.read_csv(TEST_IN)\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test  shape:\", test_df.shape)\n",
    "\n",
    "if 'clean_text' not in train_df.columns or 'clean_text' not in test_df.columns:\n",
    "    raise KeyError(\"clean_text column missing in train/test files. Run cleaning merge step first.\")\n",
    "\n",
    "# Fit vectorizer on train.clean_text only\n",
    "tfidf = TfidfVectorizer(ngram_range=ngram_range, min_df=min_df, max_df=max_df, max_features=max_features, strip_accents='unicode', lowercase=True)\n",
    "print(\"Fitting TF-IDF on train clean_text ...\")\n",
    "X_train_tfidf = tfidf.fit_transform(train_df['clean_text'].fillna('').astype(str))\n",
    "print(\"Fitted. Vocabulary size:\", len(tfidf.vocabulary_))\n",
    "\n",
    "# Transform frozen test\n",
    "X_test_tfidf = tfidf.transform(test_df['clean_text'].fillna('').astype(str))\n",
    "\n",
    "# Save vectorizer and sparse matrices\n",
    "joblib.dump(tfidf, VECT_DIR / \"tfidf_vectorizer.joblib\")\n",
    "sp.save_npz(VECT_DIR / \"X_train_tfidf.npz\", X_train_tfidf)\n",
    "sp.save_npz(VECT_DIR / \"X_test_tfidf.npz\", X_test_tfidf)\n",
    "\n",
    "print(\"\\nSaved vectorizer ->\", (VECT_DIR / \"tfidf_vectorizer.joblib\").resolve())\n",
    "print(\"Saved X_train_tfidf ->\", (VECT_DIR / \"X_train_tfidf.npz\").resolve())\n",
    "print(\"Saved X_test_tfidf  ->\", (VECT_DIR / \"X_test_tfidf.npz\").resolve())\n",
    "\n",
    "# Print shape + sparsity info\n",
    "def sparse_info(mat, name):\n",
    "    nnz = mat.nnz\n",
    "    shape = mat.shape\n",
    "    density = nnz / (shape[0]*shape[1])\n",
    "    print(f\"\\n{name} shape: {shape}, nnz: {nnz}, density: {density:.6f}\")\n",
    "\n",
    "sparse_info(X_train_tfidf, \"X_train_tfidf\")\n",
    "sparse_info(X_test_tfidf, \"X_test_tfidf\")\n",
    "\n",
    "# Show top 20 features by idf (lowest idf => most common; highest idf => rare)\n",
    "import numpy as np\n",
    "idf = np.array(tfidf.idf_)\n",
    "top_common_idx = np.argsort(idf)[:20]\n",
    "top_rare_idx   = np.argsort(-idf)[:20]\n",
    "vocab = np.array(tfidf.get_feature_names_out())\n",
    "print(\"\\nTop 20 most common n-grams (lowest idf):\")\n",
    "print(vocab[top_common_idx].tolist())\n",
    "print(\"\\nTop 20 rarest n-grams (highest idf):\")\n",
    "print(vocab[top_rare_idx].tolist())\n",
    "\n",
    "# Print sample row nonzero counts for first 6 train & test rows\n",
    "train_row_nnz = (X_train_tfidf != 0).sum(axis=1).A1\n",
    "test_row_nnz  = (X_test_tfidf != 0).sum(axis=1).A1\n",
    "print(\"\\nSample train nonzero counts (first 6 rows):\", train_row_nnz[:6].tolist())\n",
    "print(\"Sample test nonzero counts  (first 6 rows):\", test_row_nnz[:6].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69882227-7f8a-4c9c-8105-ccccc281a571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TF-IDF matrices...\n",
      "Shapes — train: (572, 2051)  test: (143, 2051)\n",
      "Fitting TruncatedSVD with n_components = 200 on train TF-IDF ...\n",
      "Saved SVD model -> C:\\Users\\AYUSH SINGH\\Documents\\GitHub\\NervSightX\\Machine learning\\dreaddit_cv_raw_splits\\svd\\tfidf_svd_200.joblib\n",
      "Saved X_train_svd -> C:\\Users\\AYUSH SINGH\\Documents\\GitHub\\NervSightX\\Machine learning\\dreaddit_cv_raw_splits\\svd\\X_train_svd_200.npy\n",
      "Saved X_test_svd  -> C:\\Users\\AYUSH SINGH\\Documents\\GitHub\\NervSightX\\Machine learning\\dreaddit_cv_raw_splits\\svd\\X_test_svd_200.npy\n",
      "\n",
      "Explained variance (sum of top 200 components): 0.6561\n",
      "Explained variance (first 10 components): [0.003290324031305188, 0.01618299569160921, 0.015497219632266246, 0.010922794256107135, 0.010451368146609291, 0.008473328006405134, 0.0070924188533376, 0.006683235428587484, 0.00654402249290138, 0.006318146476979274]\n",
      "\n",
      "SVD output shapes:\n",
      "X_train_svd shape: (572, 200)\n",
      "X_test_svd shape : (143, 200)\n",
      "\n",
      "Sample train SVD row (first 3 rows, first 6 dims):\n",
      "[[ 0.292626 -0.084161 -0.039586 -0.070352 -0.002818 -0.006795]\n",
      " [ 0.206315 -0.039224  0.032789 -0.042089  0.031427 -0.098834]\n",
      " [ 0.261952  0.106224  0.036714 -0.003368 -0.225875 -0.025593]]\n",
      "\n",
      "Sample test SVD row (first 3 rows, first 6 dims):\n",
      "[[ 2.87179e-01 -1.06008e-01 -1.25240e-02  2.92000e-04 -4.82160e-02\n",
      "   5.52900e-03]\n",
      " [ 3.00614e-01 -5.67200e-03 -6.10030e-02  6.16830e-02 -4.11990e-02\n",
      "   4.87370e-02]\n",
      " [ 2.76051e-01 -1.27460e-01 -4.00740e-02 -9.64780e-02 -3.57660e-02\n",
      "  -7.35390e-02]]\n",
      "\n",
      "Done — paste the printed output here and I will continue with lexical feature processing and fusion.\n"
     ]
    }
   ],
   "source": [
    "# Cell — TruncatedSVD on TF-IDF (200 components) — fit on train only\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "OUT_DIR = Path(\"dreaddit_cv_raw_splits\")\n",
    "VECT_DIR = OUT_DIR / \"tfidf\"\n",
    "SVD_DIR  = OUT_DIR / \"svd\"\n",
    "SVD_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# paths\n",
    "X_train_path = VECT_DIR / \"X_train_tfidf.npz\"\n",
    "X_test_path  = VECT_DIR / \"X_test_tfidf.npz\"\n",
    "tfidf_path   = VECT_DIR / \"tfidf_vectorizer.joblib\"\n",
    "\n",
    "if not X_train_path.exists() or not X_test_path.exists() or not tfidf_path.exists():\n",
    "    raise FileNotFoundError(\"TF-IDF artifacts missing. Run TF-IDF cell first.\")\n",
    "\n",
    "print(\"Loading TF-IDF matrices...\")\n",
    "X_train_tfidf = sp.load_npz(X_train_path)\n",
    "X_test_tfidf  = sp.load_npz(X_test_path)\n",
    "\n",
    "print(\"Shapes — train:\", X_train_tfidf.shape, \" test:\", X_test_tfidf.shape)\n",
    "\n",
    "# set n_components\n",
    "n_components = 200\n",
    "if n_components >= X_train_tfidf.shape[1]:\n",
    "    raise ValueError(f\"n_components ({n_components}) must be < n_features ({X_train_tfidf.shape[1]})\")\n",
    "\n",
    "print(f\"Fitting TruncatedSVD with n_components = {n_components} on train TF-IDF ...\")\n",
    "svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "X_train_svd = svd.fit_transform(X_train_tfidf)   # fit on train only\n",
    "X_test_svd  = svd.transform(X_test_tfidf)\n",
    "\n",
    "# Save model and arrays\n",
    "joblib.dump(svd, SVD_DIR / \"tfidf_svd_200.joblib\")\n",
    "np.save(SVD_DIR / \"X_train_svd_200.npy\", X_train_svd)\n",
    "np.save(SVD_DIR / \"X_test_svd_200.npy\", X_test_svd)\n",
    "\n",
    "# Print diagnostics\n",
    "print(\"Saved SVD model ->\", (SVD_DIR / \"tfidf_svd_200.joblib\").resolve())\n",
    "print(\"Saved X_train_svd ->\", (SVD_DIR / \"X_train_svd_200.npy\").resolve())\n",
    "print(\"Saved X_test_svd  ->\", (SVD_DIR / \"X_test_svd_200.npy\").resolve())\n",
    "\n",
    "explained = svd.explained_variance_ratio_.sum()\n",
    "print(f\"\\nExplained variance (sum of top {n_components} components): {explained:.4f}\")\n",
    "print(\"Explained variance (first 10 components):\", svd.explained_variance_ratio_[:10].tolist())\n",
    "\n",
    "print(\"\\nSVD output shapes:\")\n",
    "print(\"X_train_svd shape:\", X_train_svd.shape)\n",
    "print(\"X_test_svd shape :\", X_test_svd.shape)\n",
    "\n",
    "# Preview numeric ranges / sample rows\n",
    "print(\"\\nSample train SVD row (first 3 rows, first 6 dims):\")\n",
    "print(np.round(X_train_svd[:3,:6], 6))\n",
    "print(\"\\nSample test SVD row (first 3 rows, first 6 dims):\")\n",
    "print(np.round(X_test_svd[:3,:6], 6))\n",
    "\n",
    "# Save small metadata\n",
    "with open(SVD_DIR / \"svd_info.txt\", \"w\") as f:\n",
    "    f.write(f\"n_components={n_components}\\n\")\n",
    "    f.write(f\"explained_variance_sum={explained}\\n\")\n",
    "    f.write(\"explained_variance_first10=\" + \",\".join(map(str, svd.explained_variance_ratio_[:10].tolist())) + \"\\n\")\n",
    "\n",
    "print(\"\\nDone — paste the printed output here and I will continue with lexical feature processing and fusion.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccd87d39-649b-418d-acc8-b6ef8cd75dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train/test with clean_text:\n",
      " train: (572, 114)\n",
      " test : (143, 114)\n",
      "\n",
      "Lexical columns detected: 111\n",
      "Sample (first 40): ['lex_liwc_WC', 'lex_liwc_Analytic', 'lex_liwc_Clout', 'lex_liwc_Authentic', 'lex_liwc_Tone', 'lex_liwc_WPS', 'lex_liwc_Sixltr', 'lex_liwc_Dic', 'lex_liwc_function', 'lex_liwc_pronoun', 'lex_liwc_ppron', 'lex_liwc_i', 'lex_liwc_we', 'lex_liwc_you', 'lex_liwc_shehe', 'lex_liwc_they', 'lex_liwc_ipron', 'lex_liwc_article', 'lex_liwc_prep', 'lex_liwc_auxverb', 'lex_liwc_adverb', 'lex_liwc_conj', 'lex_liwc_negate', 'lex_liwc_verb', 'lex_liwc_adj', 'lex_liwc_compare', 'lex_liwc_interrog', 'lex_liwc_number', 'lex_liwc_quant', 'lex_liwc_affect', 'lex_liwc_posemo', 'lex_liwc_negemo', 'lex_liwc_anx', 'lex_liwc_anger', 'lex_liwc_sad', 'lex_liwc_social', 'lex_liwc_family', 'lex_liwc_friend', 'lex_liwc_female', 'lex_liwc_male']\n",
      "\n",
      "Missing values in train (columns with missing > 0):\n",
      "No missing values in train lexical columns\n",
      "\n",
      "Missing values in test (columns with missing > 0):\n",
      "No missing values in test lexical columns\n",
      "\n",
      "Saved lexical artifacts to: C:\\Users\\AYUSH SINGH\\Documents\\GitHub\\NervSightX\\Machine learning\\dreaddit_cv_raw_splits\\lexical\n",
      "\n",
      "Shapes after processing:\n",
      " X_train_lexical: (572, 111)\n",
      " X_test_lexical : (143, 111)\n",
      "\n",
      "Train lexical columns mean (first 10):\n",
      "lex_liwc_WC          -0.0\n",
      "lex_liwc_Analytic    -0.0\n",
      "lex_liwc_Clout       -0.0\n",
      "lex_liwc_Authentic    0.0\n",
      "lex_liwc_Tone        -0.0\n",
      "lex_liwc_WPS          0.0\n",
      "lex_liwc_Sixltr       0.0\n",
      "lex_liwc_Dic         -0.0\n",
      "lex_liwc_function     0.0\n",
      "lex_liwc_pronoun     -0.0\n",
      "\n",
      "Train lexical columns std (first 10):\n",
      "lex_liwc_WC           1.000875\n",
      "lex_liwc_Analytic     1.000875\n",
      "lex_liwc_Clout        1.000875\n",
      "lex_liwc_Authentic    1.000875\n",
      "lex_liwc_Tone         1.000875\n",
      "lex_liwc_WPS          1.000875\n",
      "lex_liwc_Sixltr       1.000875\n",
      "lex_liwc_Dic          1.000875\n",
      "lex_liwc_function     1.000875\n",
      "lex_liwc_pronoun      1.000875\n",
      "\n",
      "Sample train_lex_scaled (first 3 rows, first 6 cols):\n",
      "[[-0.518219  0.170249 -0.626649  0.750994 -0.902659 -0.553383]\n",
      " [-1.033846  0.192068 -0.111885  0.724853  0.833614 -0.623192]\n",
      " [-0.485992 -1.188926  0.534795 -1.409001 -0.874243 -0.525459]]\n",
      "\n",
      "Sample test_lex_scaled (first 3 rows, first 6 cols):\n",
      "[[-0.679352 -0.018975 -0.684383  0.930521  1.826433 -0.239241]\n",
      " [ 0.158542 -0.879321 -0.646324  0.378712 -0.597332 -0.390029]\n",
      " [ 0.094088  0.24323  -0.332176 -0.085539 -0.902659 -0.022832]]\n",
      "\n",
      "Saved lexical column list.\n"
     ]
    }
   ],
   "source": [
    "# Cell — Lexical feature processing: impute (train-only) -> StandardScale (train-only) -> save dense arrays\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "OUT_DIR = Path(\"dreaddit_cv_raw_splits\")\n",
    "TRAIN_IN = OUT_DIR / \"train_raw_with_clean_text.csv\"\n",
    "TEST_IN  = OUT_DIR / \"test_frozen_raw_with_clean_text.csv\"\n",
    "LEX_DIR  = OUT_DIR / \"lexical\"\n",
    "LEX_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Load\n",
    "train_df = pd.read_csv(TRAIN_IN)\n",
    "test_df  = pd.read_csv(TEST_IN)\n",
    "\n",
    "print(\"Loaded train/test with clean_text:\")\n",
    "print(\" train:\", train_df.shape)\n",
    "print(\" test :\", test_df.shape)\n",
    "\n",
    "# Identify lexical columns\n",
    "liwc_cols = [c for c in train_df.columns if c.startswith(\"lex_liwc_\")]\n",
    "dal_cols  = [c for c in train_df.columns if c.startswith(\"lex_dal_\")]\n",
    "syntax_cols = [c for c in train_df.columns if c.startswith(\"syntax_\") or c in (\"syntax_ari\",\"syntax_fk_grade\")]\n",
    "social_cols = [c for c in train_df.columns if c.startswith(\"social_\")]\n",
    "extra = [c for c in (\"sentiment\",\"token_len\",\"char_len\") if c in train_df.columns]\n",
    "\n",
    "lexical_cols = liwc_cols + dal_cols + syntax_cols + social_cols + extra\n",
    "# dedupe while preserving order\n",
    "seen = set(); lexical_cols = [x for x in lexical_cols if not (x in seen or seen.add(x))]\n",
    "\n",
    "print(\"\\nLexical columns detected:\", len(lexical_cols))\n",
    "print(\"Sample (first 40):\", lexical_cols[:40])\n",
    "\n",
    "# Build matrices (keep orig_index for traceability)\n",
    "train_lex = train_df[lexical_cols].copy()\n",
    "test_lex  = test_df[lexical_cols].copy()\n",
    "\n",
    "# Missingness report\n",
    "missing_train = train_lex.isna().sum().sort_values(ascending=False)\n",
    "missing_test  = test_lex.isna().sum().sort_values(ascending=False)\n",
    "print(\"\\nMissing values in train (columns with missing > 0):\")\n",
    "print(missing_train[missing_train>0].to_string() if missing_train.sum()>0 else \"No missing values in train lexical columns\")\n",
    "print(\"\\nMissing values in test (columns with missing > 0):\")\n",
    "print(missing_test[missing_test>0].to_string() if missing_test.sum()>0 else \"No missing values in test lexical columns\")\n",
    "\n",
    "# Imputer fit on train only\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "imputer.fit(train_lex)\n",
    "train_lex_imp = pd.DataFrame(imputer.transform(train_lex), columns=lexical_cols)\n",
    "test_lex_imp  = pd.DataFrame(imputer.transform(test_lex), columns=lexical_cols)\n",
    "\n",
    "# Save imputer\n",
    "joblib.dump(imputer, LEX_DIR / \"lex_imputer.joblib\")\n",
    "\n",
    "# Scaler fit on train only\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_lex_imp)\n",
    "train_lex_scaled = pd.DataFrame(scaler.transform(train_lex_imp), columns=lexical_cols)\n",
    "test_lex_scaled  = pd.DataFrame(scaler.transform(test_lex_imp), columns=lexical_cols)\n",
    "\n",
    "# Save scaler and arrays\n",
    "joblib.dump(scaler, LEX_DIR / \"lex_scaler.joblib\")\n",
    "np.save(LEX_DIR / \"X_train_lexical.npy\", train_lex_scaled.values)\n",
    "np.save(LEX_DIR / \"X_test_lexical.npy\", test_lex_scaled.values)\n",
    "\n",
    "print(\"\\nSaved lexical artifacts to:\", LEX_DIR.resolve())\n",
    "print(\"\\nShapes after processing:\")\n",
    "print(\" X_train_lexical:\", train_lex_scaled.shape)\n",
    "print(\" X_test_lexical :\", test_lex_scaled.shape)\n",
    "\n",
    "# Per-column mean/std (should be ~0/1 on train)\n",
    "means = train_lex_scaled.mean().round(6)\n",
    "stds  = train_lex_scaled.std().round(6)\n",
    "print(\"\\nTrain lexical columns mean (first 10):\")\n",
    "print(means.head(10).to_string())\n",
    "print(\"\\nTrain lexical columns std (first 10):\")\n",
    "print(stds.head(10).to_string())\n",
    "\n",
    "# Print sample rows\n",
    "print(\"\\nSample train_lex_scaled (first 3 rows, first 6 cols):\")\n",
    "print(np.round(train_lex_scaled.values[:3,:6], 6))\n",
    "print(\"\\nSample test_lex_scaled (first 3 rows, first 6 cols):\")\n",
    "print(np.round(test_lex_scaled.values[:3,:6], 6))\n",
    "\n",
    "# Save column list\n",
    "pd.Series(lexical_cols).to_csv(LEX_DIR / \"lexical_columns_list.csv\", index=False)\n",
    "print(\"\\nSaved lexical column list.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00765c85-f983-44fc-9f48-29f4898c1bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded arrays:\n",
      " X_train_svd: (572, 200)\n",
      " X_test_svd : (143, 200)\n",
      " X_train_lex: (572, 111)\n",
      " X_test_lex : (143, 111)\n",
      "\n",
      "Fused arrays saved to: C:\\Users\\AYUSH SINGH\\Documents\\GitHub\\NervSightX\\Machine learning\\dreaddit_cv_raw_splits\\fused\n",
      "\n",
      "Fused shapes:\n",
      " X_train_fused: (572, 311)\n",
      " X_test_fused : (143, 311)\n",
      "\n",
      "Dtypes: float64 float64\n",
      "\n",
      "Approx memory (bytes):\n",
      " train: 1423136\n",
      " test : 355784\n",
      "\n",
      "Sample X_train_fused first 3 rows (first 8 dims):\n",
      "[[ 0.292626 -0.084161 -0.039586 -0.070352 -0.002818 -0.006795 -0.015907\n",
      "   0.042966]\n",
      " [ 0.206315 -0.039224  0.032789 -0.042089  0.031427 -0.098834 -0.082695\n",
      "  -0.057838]\n",
      " [ 0.261952  0.106224  0.036714 -0.003368 -0.225875 -0.025593  0.047436\n",
      "  -0.089772]]\n",
      "\n",
      "Sample X_test_fused first 3 rows (first 8 dims):\n",
      "[[ 2.87179e-01 -1.06008e-01 -1.25240e-02  2.92000e-04 -4.82160e-02\n",
      "   5.52900e-03  7.96800e-03 -5.59310e-02]\n",
      " [ 3.00614e-01 -5.67200e-03 -6.10030e-02  6.16830e-02 -4.11990e-02\n",
      "   4.87370e-02  2.52660e-02 -3.05440e-02]\n",
      " [ 2.76051e-01 -1.27460e-01 -4.00740e-02 -9.64780e-02 -3.57660e-02\n",
      "  -7.35390e-02 -4.48380e-02  2.57530e-02]]\n",
      "\n",
      "Feature counts -> SVD: 200 LEX: 111 FUSED: 311\n"
     ]
    }
   ],
   "source": [
    "# Cell — Feature fusion: concatenate SVD TF-IDF (200) + lexical dense (111) -> fused dense features\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "OUT_DIR = Path(\"dreaddit_cv_raw_splits\")\n",
    "SVD_DIR = OUT_DIR / \"svd\"\n",
    "LEX_DIR = OUT_DIR / \"lexical\"\n",
    "FUSED_DIR = OUT_DIR / \"fused\"\n",
    "FUSED_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# load arrays\n",
    "X_train_svd = np.load(SVD_DIR / \"X_train_svd_200.npy\")\n",
    "X_test_svd  = np.load(SVD_DIR / \"X_test_svd_200.npy\")\n",
    "X_train_lex = np.load(LEX_DIR / \"X_train_lexical.npy\")\n",
    "X_test_lex  = np.load(LEX_DIR / \"X_test_lexical.npy\")\n",
    "\n",
    "print(\"Loaded arrays:\")\n",
    "print(\" X_train_svd:\", X_train_svd.shape)\n",
    "print(\" X_test_svd :\", X_test_svd.shape)\n",
    "print(\" X_train_lex:\", X_train_lex.shape)\n",
    "print(\" X_test_lex :\", X_test_lex.shape)\n",
    "\n",
    "# sanity check: number of rows must match\n",
    "assert X_train_svd.shape[0] == X_train_lex.shape[0], \"Train row mismatch between SVD and lexical\"\n",
    "assert X_test_svd.shape[0]  == X_test_lex.shape[0],  \"Test row mismatch between SVD and lexical\"\n",
    "\n",
    "# Concatenate horizontally\n",
    "X_train_fused = np.hstack([X_train_svd, X_train_lex])\n",
    "X_test_fused  = np.hstack([X_test_svd, X_test_lex])\n",
    "\n",
    "# Save fused arrays\n",
    "np.save(FUSED_DIR / \"X_train_fused.npy\", X_train_fused)\n",
    "np.save(FUSED_DIR / \"X_test_fused.npy\", X_test_fused)\n",
    "\n",
    "# Save manifest\n",
    "manifest = {\n",
    "    \"X_train_svd_shape\": X_train_svd.shape,\n",
    "    \"X_train_lex_shape\": X_train_lex.shape,\n",
    "    \"X_train_fused_shape\": X_train_fused.shape,\n",
    "    \"X_test_svd_shape\": X_test_svd.shape,\n",
    "    \"X_test_lex_shape\": X_test_lex.shape,\n",
    "    \"X_test_fused_shape\": X_test_fused.shape,\n",
    "    \"svd_components\": X_train_svd.shape[1],\n",
    "    \"lexical_features\": X_train_lex.shape[1],\n",
    "    \"fused_features\": X_train_fused.shape[1]\n",
    "}\n",
    "pd.Series(manifest).to_csv(FUSED_DIR / \"fused_manifest.csv\", index=True)\n",
    "\n",
    "# Print info\n",
    "def approx_bytes(arr):\n",
    "    return arr.nbytes\n",
    "\n",
    "print(\"\\nFused arrays saved to:\", FUSED_DIR.resolve())\n",
    "print(\"\\nFused shapes:\")\n",
    "print(\" X_train_fused:\", X_train_fused.shape)\n",
    "print(\" X_test_fused :\", X_test_fused.shape)\n",
    "print(\"\\nDtypes:\", X_train_fused.dtype, X_test_fused.dtype)\n",
    "print(\"\\nApprox memory (bytes):\")\n",
    "print(\" train:\", approx_bytes(X_train_fused))\n",
    "print(\" test :\", approx_bytes(X_test_fused))\n",
    "\n",
    "# Print small numeric preview\n",
    "print(\"\\nSample X_train_fused first 3 rows (first 8 dims):\")\n",
    "print(np.round(X_train_fused[:3,:8], 6))\n",
    "print(\"\\nSample X_test_fused first 3 rows (first 8 dims):\")\n",
    "print(np.round(X_test_fused[:3,:8], 6))\n",
    "\n",
    "# Show fused features count\n",
    "print(\"\\nFeature counts -> SVD:\", X_train_svd.shape[1], \"LEX:\", X_train_lex.shape[1], \"FUSED:\", X_train_fused.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3471c861-9cd3-45df-ae0c-3b0d042e9deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded fused:\n",
      " X_train_fused: (572, 311)\n",
      " X_test_fused : (143, 311)\n",
      "Loaded labels: (572,)\n",
      "\n",
      "Fitting L1 Logistic Regression on fused features...\n",
      "Done fitting.\n",
      "\n",
      "Saved selected feature arrays and model artifacts to: C:\\Users\\AYUSH SINGH\\Documents\\GitHub\\NervSightX\\Machine learning\\dreaddit_cv_raw_splits\\selected_features\n",
      "\n",
      "Feature selection complete:\n",
      "  Total features: 311\n",
      "  Selected features: 34\n",
      "  Reduction: 277\n",
      "\n",
      "First 20 selected feature indices: [202 204 211 213 216 221 222 227 230 231 232 236 239 241 243 246 248 250\n",
      " 251 261]\n",
      "\n",
      "Preview of X_train_selected (first 3 rows, first 8 dims):\n",
      "[[-0.626649 -0.902659 -0.810858 -0.385364  0.347508  0.31842  -0.478641\n",
      "   0.763188]\n",
      " [-0.111885  0.833614  1.661692 -0.385364 -0.694275 -1.391037 -1.220262\n",
      "   0.249805]\n",
      " [ 0.534795 -0.874243  0.443465 -0.385364  1.171244  1.266168 -0.489013\n",
      "  -0.690551]]\n",
      "\n",
      "Preview of X_test_selected (first 3 rows, first 8 dims):\n",
      "[[-0.684383  1.826433  0.330666  0.407265  0.514072 -1.068101 -0.421593\n",
      "  -0.690551]\n",
      " [-0.646324 -0.597332  0.628455  0.746963  0.180944  0.781763 -0.649784\n",
      "  -0.690551]\n",
      " [-0.332176 -0.902659  0.181772 -0.385364 -0.794214  0.072707 -1.220262\n",
      "  -0.121255]]\n"
     ]
    }
   ],
   "source": [
    "# Cell — L1 Logistic Regression Feature Selection on fused features\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "OUT_DIR = Path(\"dreaddit_cv_raw_splits\")\n",
    "FUSED_DIR = OUT_DIR / \"fused\"\n",
    "SEL_DIR = OUT_DIR / \"selected_features\"\n",
    "SEL_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Load fused features\n",
    "X_train_fused = np.load(FUSED_DIR / \"X_train_fused.npy\")\n",
    "X_test_fused  = np.load(FUSED_DIR / \"X_test_fused.npy\")\n",
    "\n",
    "print(\"Loaded fused:\")\n",
    "print(\" X_train_fused:\", X_train_fused.shape)\n",
    "print(\" X_test_fused :\", X_test_fused.shape)\n",
    "\n",
    "# Load labels\n",
    "train_df = pd.read_csv(OUT_DIR / \"train_raw_with_clean_text.csv\")\n",
    "y_train = train_df[\"label\"].values\n",
    "\n",
    "print(\"Loaded labels:\", y_train.shape)\n",
    "\n",
    "# Fit L1-regularized Logistic Regression (Linear Model)\n",
    "# Smaller C => stronger regularization => more feature selection\n",
    "clf = LogisticRegression(\n",
    "    penalty=\"l1\",\n",
    "    solver=\"liblinear\",\n",
    "    C=0.1,\n",
    "    max_iter=2000,\n",
    "    class_weight=\"balanced\"\n",
    ")\n",
    "\n",
    "print(\"\\nFitting L1 Logistic Regression on fused features...\")\n",
    "clf.fit(X_train_fused, y_train)\n",
    "print(\"Done fitting.\")\n",
    "\n",
    "# Select features with non-zero coefficients\n",
    "selector = SelectFromModel(clf, prefit=True, threshold=\"mean\")\n",
    "\n",
    "selected_mask = selector.get_support()\n",
    "X_train_sel = selector.transform(X_train_fused)\n",
    "X_test_sel  = selector.transform(X_test_fused)\n",
    "\n",
    "# Save artifacts\n",
    "np.save(SEL_DIR / \"X_train_fused_selected.npy\", X_train_sel)\n",
    "np.save(SEL_DIR / \"X_test_fused_selected.npy\", X_test_sel)\n",
    "np.save(SEL_DIR / \"selected_mask.npy\", selected_mask)\n",
    "\n",
    "joblib.dump(selector, SEL_DIR / \"selector_L1.joblib\")\n",
    "joblib.dump(clf, SEL_DIR / \"L1_logistic_model.joblib\")\n",
    "\n",
    "print(\"\\nSaved selected feature arrays and model artifacts to:\", SEL_DIR.resolve())\n",
    "\n",
    "# Report selected features\n",
    "n_total = X_train_fused.shape[1]\n",
    "n_selected = X_train_sel.shape[1]\n",
    "print(f\"\\nFeature selection complete:\")\n",
    "print(f\"  Total features: {n_total}\")\n",
    "print(f\"  Selected features: {n_selected}\")\n",
    "print(f\"  Reduction: {n_total - n_selected}\")\n",
    "\n",
    "selected_indices = np.where(selected_mask)[0]\n",
    "print(\"\\nFirst 20 selected feature indices:\", selected_indices[:20])\n",
    "\n",
    "# Preview numeric values\n",
    "print(\"\\nPreview of X_train_selected (first 3 rows, first 8 dims):\")\n",
    "print(np.round(X_train_sel[:3, :8], 6))\n",
    "\n",
    "print(\"\\nPreview of X_test_selected (first 3 rows, first 8 dims):\")\n",
    "print(np.round(X_test_sel[:3, :8], 6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4118ffde-dc2f-4b1f-b119-fb12eb74a8d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (1830655867.py, line 139)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[12], line 139\u001b[1;36m\u001b[0m\n\u001b[1;33m    pipe = Pipeline([(\"clf\", LogisticRegression(class_weight='balanced'Y, solver='liblinear', max_iter=2000))])\u001b[0m\n\u001b[1;37m                                                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "# Cell — Prepare per-fold selected feature datasets (class_weight strategy)\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "ROOT = Path(\"dreaddit_cv_raw_splits\")\n",
    "FOLDS_RAW = ROOT / \"folds_raw\"\n",
    "TFIDF_DIR = ROOT / \"tfidf\"\n",
    "SVD_DIR   = ROOT / \"svd\"\n",
    "LEX_DIR   = ROOT / \"lexical\"\n",
    "SEL_DIR   = ROOT / \"selected_features\"\n",
    "TRAIN_WITH_CLEAN = ROOT / \"train_raw_with_clean_text.csv\"\n",
    "\n",
    "OUT_DIR = ROOT / \"folds_selected\"\n",
    "OUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# artifacts (must exist)\n",
    "tfidf_path = TFIDF_DIR / \"tfidf_vectorizer.joblib\"\n",
    "svd_path   = SVD_DIR / \"tfidf_svd_200.joblib\"\n",
    "imputer_path = LEX_DIR / \"lex_imputer.joblib\"\n",
    "scaler_path  = LEX_DIR / \"lex_scaler.joblib\"\n",
    "selector_path = SEL_DIR / \"selector_L1.joblib\"\n",
    "lex_cols_path = LEX_DIR / \"lexical_columns_list.csv\"\n",
    "\n",
    "for p in (tfidf_path, svd_path, imputer_path, scaler_path, selector_path, lex_cols_path, TRAIN_WITH_CLEAN):\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing required artifact: {p}\")\n",
    "\n",
    "# load artifacts\n",
    "tfidf = joblib.load(tfidf_path)\n",
    "svd   = joblib.load(svd_path)\n",
    "imputer = joblib.load(imputer_path)\n",
    "scaler  = joblib.load(scaler_path)\n",
    "selector = joblib.load(selector_path)\n",
    "lex_cols = pd.read_csv(lex_cols_path, header=None)[0].tolist()\n",
    "\n",
    "# load master train_with_clean (to get clean_text and lexical columns)\n",
    "train_with_clean = pd.read_csv(TRAIN_WITH_CLEAN)\n",
    "train_with_clean_indexed = train_with_clean.set_index(\"orig_index\")\n",
    "print(\"Loaded train_with_clean:\", train_with_clean.shape)\n",
    "\n",
    "# list fold files\n",
    "fold_files = sorted([p for p in (FOLDS_RAW).glob(\"fold_*_train_raw.csv\")])\n",
    "if not fold_files:\n",
    "    raise FileNotFoundError(f\"No fold_train files found in {FOLDS_RAW}\")\n",
    "\n",
    "manifest = []\n",
    "\n",
    "for train_file in fold_files:\n",
    "    # infer fold number\n",
    "    name = train_file.stem  # e.g., fold_01_train_raw\n",
    "    fold_no = name.split(\"_\")[1]\n",
    "    val_file = FOLDS_RAW / f\"fold_{fold_no}_val_raw.csv\"\n",
    "    if not val_file.exists():\n",
    "        raise FileNotFoundError(f\"Expected val file for fold {fold_no} at {val_file}\")\n",
    "\n",
    "    # load fold train & val raw (these include orig_index and label)\n",
    "    df_tr = pd.read_csv(train_file)\n",
    "    df_val = pd.read_csv(val_file)\n",
    "\n",
    "    # fetch clean_text and lexical columns from master train_with_clean using orig_index\n",
    "    # ensure orig_index exists\n",
    "    if 'orig_index' not in df_tr.columns or 'orig_index' not in df_val.columns:\n",
    "        raise KeyError(\"fold files must include 'orig_index' column\")\n",
    "\n",
    "    # helper to build fused features for a subset\n",
    "    def build_fused_selected(sub_df):\n",
    "        # sub_df has orig_index, label and numeric columns\n",
    "        # get clean_text rows by orig_index from master\n",
    "        idx = sub_df['orig_index'].values\n",
    "        clean_texts = train_with_clean_indexed.loc[idx, 'clean_text'].astype(str).values\n",
    "\n",
    "        # TF-IDF transform -> SVD transform\n",
    "        X_tfidf = tfidf.transform(clean_texts)\n",
    "        X_svd = svd.transform(X_tfidf)   # dense (n_rows, n_svd)\n",
    "\n",
    "        # lexical matrix: pick lex_cols from sub_df (ensure columns present)\n",
    "        missing_lex = [c for c in lex_cols if c not in sub_df.columns]\n",
    "        if missing_lex:\n",
    "            raise KeyError(f\"Missing lexical columns in fold file: {missing_lex[:5]}... (total {len(missing_lex)})\")\n",
    "        lex_mat = sub_df[lex_cols].values\n",
    "\n",
    "        # impute then scale using saved imputer/scaler (they were fitted on global train)\n",
    "        lex_imp = imputer.transform(lex_mat)\n",
    "        lex_scaled = scaler.transform(lex_imp)\n",
    "\n",
    "        # fuse\n",
    "        fused = np.hstack([X_svd, lex_scaled])\n",
    "\n",
    "        # apply selector (SelectFromModel) to get selected dims\n",
    "        fused_selected = selector.transform(fused)\n",
    "\n",
    "        return fused_selected\n",
    "\n",
    "    X_tr_sel = build_fused_selected(df_tr)\n",
    "    X_val_sel = build_fused_selected(df_val)\n",
    "\n",
    "    # Save arrays + CSVs with metadata (orig_index + label + selected feature columns)\n",
    "    fold_out_dir = OUT_DIR / f\"fold_{fold_no}\"\n",
    "    fold_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    np.save(fold_out_dir / f\"fold_{fold_no}_train_selected.npy\", X_tr_sel)\n",
    "    np.save(fold_out_dir / f\"fold_{fold_no}_val_selected.npy\", X_val_sel)\n",
    "\n",
    "    # Save CSVs with orig_index and label plus selected features (column names x0,x1,...)\n",
    "    sel_cols = [f\"x{i}\" for i in range(X_tr_sel.shape[1])]\n",
    "    train_csv = pd.concat([df_tr[['orig_index','label']].reset_index(drop=True),\n",
    "                           pd.DataFrame(X_tr_sel, columns=sel_cols)], axis=1)\n",
    "    val_csv   = pd.concat([df_val[['orig_index','label']].reset_index(drop=True),\n",
    "                           pd.DataFrame(X_val_sel, columns=sel_cols)], axis=1)\n",
    "\n",
    "    train_csv.to_csv(fold_out_dir / f\"fold_{fold_no}_train_selected.csv\", index=False)\n",
    "    val_csv.to_csv(fold_out_dir / f\"fold_{fold_no}_val_selected.csv\", index=False)\n",
    "\n",
    "    manifest.append({\n",
    "        \"fold\": fold_no,\n",
    "        \"train_rows\": X_tr_sel.shape[0],\n",
    "        \"val_rows\": X_val_sel.shape[0],\n",
    "        \"train_selected_path\": str(fold_out_dir / f\"fold_{fold_no}_train_selected.npy\"),\n",
    "        \"val_selected_path\": str(fold_out_dir / f\"fold_{fold_no}_val_selected.npy\"),\n",
    "        \"train_csv\": str(fold_out_dir / f\"fold_{fold_no}_train_selected.csv\"),\n",
    "        \"val_csv\": str(fold_out_dir / f\"fold_{fold_no}_val_selected.csv\")\n",
    "    })\n",
    "\n",
    "    print(f\"Fold {fold_no}: saved selected train ({X_tr_sel.shape}) and val ({X_val_sel.shape})\")\n",
    "\n",
    "# Save manifest\n",
    "manifest_df = pd.DataFrame(manifest)\n",
    "manifest_df.to_csv(OUT_DIR / \"folds_selected_manifest.csv\", index=False)\n",
    "print(\"\\nSaved folds_selected_manifest ->\", (OUT_DIR / \"folds_selected_manifest.csv\").resolve())\n",
    "\n",
    "# Save an unfitted pipeline stub with class_weight='balanced' logistic regression\n",
    "IMB_DIR = ROOT / \"imbalance\"\n",
    "IMB_DIR.mkdir(exist_ok=True, parents=True)\n",
    "pipe = Pipeline([(\"clf\", LogisticRegression(class_weight='balanced'Y, solver='liblinear', max_iter=2000))])\n",
    "joblib.dump(pipe, IMB_DIR / \"pipeline_class_weight_balanced_unfitted.joblib\")\n",
    "print(\"Saved unfitted pipeline with class_weight='balanced' ->\", (IMB_DIR / \"pipeline_class_weight_balanced_unfitted.joblib\").resolve())\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nSummary of folds prepared:\")\n",
    "print(manifest_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nDone. You now have per-fold selected datasets (unfitted). The saved pipeline is configured to use class_weight='balanced' and is not fitted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35e95166-a6fb-4b99-b2d1-c8ca0af925bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputer expects features: 111\n",
      "Len(lex_cols) from lexical_columns_list.csv: 112\n",
      "Loaded train_master shape: (572, 114)\n",
      "Master lexical columns available: 111 of 112\n",
      "Trimming lex_cols -> 111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AYUSH SINGH\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but SimpleImputer was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\AYUSH SINGH\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\AYUSH SINGH\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but SimpleImputer was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\AYUSH SINGH\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\AYUSH SINGH\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but SimpleImputer was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\AYUSH SINGH\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 01 -> saved selected: train (457, 34), val (115, 34) (missing_rows train=0, val=0)\n",
      "Fold 02 -> saved selected: train (457, 34), val (115, 34) (missing_rows train=0, val=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AYUSH SINGH\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but SimpleImputer was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\AYUSH SINGH\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\AYUSH SINGH\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but SimpleImputer was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\AYUSH SINGH\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\AYUSH SINGH\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but SimpleImputer was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\AYUSH SINGH\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\AYUSH SINGH\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but SimpleImputer was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\AYUSH SINGH\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\AYUSH SINGH\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but SimpleImputer was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\AYUSH SINGH\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 03 -> saved selected: train (458, 34), val (114, 34) (missing_rows train=0, val=0)\n",
      "Fold 04 -> saved selected: train (458, 34), val (114, 34) (missing_rows train=0, val=0)\n",
      "Fold 05 -> saved selected: train (458, 34), val (114, 34) (missing_rows train=0, val=0)\n",
      "\n",
      "Saved folds_selected_manifest -> C:\\Users\\AYUSH SINGH\\Documents\\GitHub\\NervSightX\\Machine learning\\dreaddit_cv_raw_splits\\folds_selected\\folds_selected_manifest.csv\n",
      "Done. Per-fold selected datasets recreated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AYUSH SINGH\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but SimpleImputer was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\AYUSH SINGH\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\AYUSH SINGH\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but SimpleImputer was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\AYUSH SINGH\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Fixed robust fold-selection cell (no nonlocal, returns missing_count)\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "ROOT = Path(\"dreaddit_cv_raw_splits\")\n",
    "FOLDS_RAW = ROOT / \"folds_raw\"\n",
    "TFIDF_DIR = ROOT / \"tfidf\"\n",
    "SVD_DIR   = ROOT / \"svd\"\n",
    "LEX_DIR   = ROOT / \"lexical\"\n",
    "SEL_DIR   = ROOT / \"selected_features\"\n",
    "TRAIN_WITH_CLEAN = ROOT / \"train_raw_with_clean_text.csv\"\n",
    "OUT_DIR = ROOT / \"folds_selected\"\n",
    "OUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# artifact paths\n",
    "tfidf_path = TFIDF_DIR / \"tfidf_vectorizer.joblib\"\n",
    "svd_path   = SVD_DIR / \"tfidf_svd_200.joblib\"\n",
    "imputer_path = LEX_DIR / \"lex_imputer.joblib\"\n",
    "scaler_path  = LEX_DIR / \"lex_scaler.joblib\"\n",
    "selector_path = SEL_DIR / \"selector_L1.joblib\"\n",
    "lex_cols_path = LEX_DIR / \"lexical_columns_list.csv\"\n",
    "\n",
    "for p in (tfidf_path, svd_path, imputer_path, scaler_path, selector_path, lex_cols_path, TRAIN_WITH_CLEAN):\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing required artifact: {p}\")\n",
    "\n",
    "# load artifacts\n",
    "tfidf = joblib.load(tfidf_path)\n",
    "svd   = joblib.load(svd_path)\n",
    "imputer = joblib.load(imputer_path)\n",
    "scaler  = joblib.load(scaler_path)\n",
    "selector = joblib.load(selector_path)\n",
    "lex_cols = pd.read_csv(lex_cols_path, header=None)[0].tolist()\n",
    "\n",
    "# get imputer expected feature count\n",
    "try:\n",
    "    imp_n_features = int(imputer.statistics_.shape[0])\n",
    "except Exception:\n",
    "    imp_n_features = len(lex_cols)\n",
    "\n",
    "print(\"Imputer expects features:\", imp_n_features)\n",
    "print(\"Len(lex_cols) from lexical_columns_list.csv:\", len(lex_cols))\n",
    "\n",
    "# load master\n",
    "train_master = pd.read_csv(TRAIN_WITH_CLEAN)\n",
    "train_master['orig_index'] = train_master['orig_index'].astype(int)\n",
    "train_master_indexed = train_master.set_index('orig_index')\n",
    "print(\"Loaded train_master shape:\", train_master.shape)\n",
    "print(\"Master lexical columns available:\", len([c for c in train_master.columns if c in lex_cols]), \"of\", len(lex_cols))\n",
    "\n",
    "# align lex_cols to imputer expectation\n",
    "aligned_lex_cols = list(lex_cols)\n",
    "if len(aligned_lex_cols) > imp_n_features:\n",
    "    aligned_lex_cols = aligned_lex_cols[:imp_n_features]\n",
    "    print(f\"Trimming lex_cols -> {len(aligned_lex_cols)}\")\n",
    "elif len(aligned_lex_cols) < imp_n_features:\n",
    "    pad_count = imp_n_features - len(aligned_lex_cols)\n",
    "    pad_names = [f\"__pad_lex_{i}\" for i in range(pad_count)]\n",
    "    aligned_lex_cols += pad_names\n",
    "    print(f\"Padding lex_cols with {pad_count} zero-columns -> total {len(aligned_lex_cols)}\")\n",
    "\n",
    "# prepare fold files\n",
    "fold_train_files = sorted([p for p in FOLDS_RAW.glob(\"fold_*_train_raw.csv\")])\n",
    "if not fold_train_files:\n",
    "    raise FileNotFoundError(f\"No fold_train files found in {FOLDS_RAW}\")\n",
    "\n",
    "manifest = []\n",
    "any_missing = []\n",
    "\n",
    "def build_fused_selected_from_master(sub_df, fold_no):\n",
    "    # returns fused_selected array and missing_count\n",
    "    # get orig_index as ints where possible\n",
    "    try:\n",
    "        idx = sub_df['orig_index'].astype(int).values\n",
    "    except Exception:\n",
    "        idx = sub_df['orig_index'].values\n",
    "\n",
    "    # lookup rows in master (order preserved)\n",
    "    master_rows = train_master_indexed.reindex(idx)\n",
    "\n",
    "    # handle missing clean_text / lexical gracefully\n",
    "    if 'clean_text' in master_rows.columns:\n",
    "        missing_mask = master_rows['clean_text'].isna()\n",
    "        missing_count = int(missing_mask.sum())\n",
    "        if missing_count > 0:\n",
    "            any_missing.append((fold_no, missing_count))\n",
    "            master_rows['clean_text'] = master_rows['clean_text'].fillna('')\n",
    "    else:\n",
    "        master_rows['clean_text'] = ''\n",
    "        missing_count = 0\n",
    "\n",
    "    clean_texts = master_rows['clean_text'].astype(str).values\n",
    "\n",
    "    # TF-IDF -> SVD\n",
    "    X_tfidf = tfidf.transform(clean_texts)\n",
    "    X_svd = svd.transform(X_tfidf)\n",
    "\n",
    "    # build aligned lexical matrix matching imp_n_features\n",
    "    n_rows = len(master_rows)\n",
    "    lex_mat = np.zeros((n_rows, imp_n_features), dtype=float)\n",
    "    for j, col in enumerate(aligned_lex_cols):\n",
    "        if col in master_rows.columns:\n",
    "            lex_mat[:, j] = pd.to_numeric(master_rows[col].fillna(0), errors='coerce').fillna(0).astype(float).values\n",
    "        else:\n",
    "            lex_mat[:, j] = 0.0\n",
    "\n",
    "    # impute & scale\n",
    "    lex_imp = imputer.transform(lex_mat)\n",
    "    lex_scaled = scaler.transform(lex_imp)\n",
    "\n",
    "    # fuse & select\n",
    "    fused = np.hstack([X_svd, lex_scaled])\n",
    "    fused_selected = selector.transform(fused)\n",
    "    return fused_selected, missing_count\n",
    "\n",
    "for train_file in fold_train_files:\n",
    "    name = train_file.stem\n",
    "    fold_no = name.split(\"_\")[1]\n",
    "    val_file = FOLDS_RAW / f\"fold_{fold_no}_val_raw.csv\"\n",
    "    if not val_file.exists():\n",
    "        raise FileNotFoundError(f\"Missing validation file for fold {fold_no}: {val_file}\")\n",
    "\n",
    "    df_tr = pd.read_csv(train_file)\n",
    "    df_val = pd.read_csv(val_file)\n",
    "    if 'orig_index' not in df_tr.columns or 'orig_index' not in df_val.columns:\n",
    "        raise KeyError(\"fold files must include 'orig_index' column\")\n",
    "\n",
    "    X_tr_sel, miss_tr = build_fused_selected_from_master(df_tr, fold_no)\n",
    "    X_val_sel, miss_val = build_fused_selected_from_master(df_val, fold_no)\n",
    "\n",
    "    # save outputs\n",
    "    fold_out = OUT_DIR / f\"fold_{fold_no}\"\n",
    "    fold_out.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    np.save(fold_out / f\"fold_{fold_no}_train_selected.npy\", X_tr_sel)\n",
    "    np.save(fold_out / f\"fold_{fold_no}_val_selected.npy\", X_val_sel)\n",
    "\n",
    "    sel_cols = [f\"x{i}\" for i in range(X_tr_sel.shape[1])]\n",
    "    train_csv = pd.concat([df_tr[['orig_index','label']].reset_index(drop=True),\n",
    "                           pd.DataFrame(X_tr_sel, columns=sel_cols)], axis=1)\n",
    "    val_csv   = pd.concat([df_val[['orig_index','label']].reset_index(drop=True),\n",
    "                           pd.DataFrame(X_val_sel, columns=sel_cols)], axis=1)\n",
    "\n",
    "    train_csv.to_csv(fold_out / f\"fold_{fold_no}_train_selected.csv\", index=False)\n",
    "    val_csv.to_csv(fold_out / f\"fold_{fold_no}_val_selected.csv\", index=False)\n",
    "\n",
    "    manifest.append({\n",
    "        \"fold\": fold_no,\n",
    "        \"train_rows\": X_tr_sel.shape[0],\n",
    "        \"val_rows\": X_val_sel.shape[0],\n",
    "        \"selected_features\": X_tr_sel.shape[1],\n",
    "        \"train_csv\": str(fold_out / f\"fold_{fold_no}_train_selected.csv\"),\n",
    "        \"val_csv\": str(fold_out / f\"fold_{fold_no}_val_selected.csv\")\n",
    "    })\n",
    "\n",
    "    print(f\"Fold {fold_no} -> saved selected: train {X_tr_sel.shape}, val {X_val_sel.shape} (missing_rows train={miss_tr}, val={miss_val})\")\n",
    "\n",
    "# save manifest\n",
    "manifest_df = pd.DataFrame(manifest)\n",
    "manifest_df.to_csv(OUT_DIR / \"folds_selected_manifest.csv\", index=False)\n",
    "\n",
    "print(\"\\nSaved folds_selected_manifest ->\", (OUT_DIR / \"folds_selected_manifest.csv\").resolve())\n",
    "if any_missing:\n",
    "    print(\"Warning: some folds had missing orig_index rows in the master. Details (fold_no, missing_count):\")\n",
    "    print(any_missing)\n",
    "print(\"Done. Per-fold selected datasets recreated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0de2b6-f968-4a6e-be7c-a65abe1a386d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
