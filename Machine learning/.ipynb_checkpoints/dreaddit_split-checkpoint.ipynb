{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768c9ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c23d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Paths & load\n",
    "DATA_PATH = 'dreaddit_StressAnalysis - Sheet1.csv'\n",
    "OUT_DIR = Path('dreaddit_eda_outputs')\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "print('rows, cols:', df.shape)\n",
    "print('columns:', df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6915c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Select required columns\n",
    "keep_cols = [c for c in df.columns if (\n",
    "    c == 'text' or c == 'label' or c.startswith('lex_liwc_') or c.startswith('lex_dal_') or c.startswith('syntax_') or c == 'sentiment' or c.startswith('social_')\n",
    ")]\n",
    "\n",
    "expected_extra = ['confidence', 'post_id', 'id', 'subreddit', 'social_karma']\n",
    "for c in expected_extra:\n",
    "    if c in df.columns and c not in keep_cols:\n",
    "        keep_cols.append(c)\n",
    "\n",
    "eda_df = df[keep_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b5d82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Basic statistics\n",
    "summary = eda_df.describe(include='all').transpose()\n",
    "summary.to_csv(OUT_DIR / 'basic_summary.csv')\n",
    "\n",
    "class_counts = eda_df['label'].value_counts(dropna=False)\n",
    "class_counts.to_csv(OUT_DIR / 'label_counts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ceef7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Class balance\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.barplot(x=class_counts.index, y=class_counts.values)\n",
    "plt.title('Class distribution')\n",
    "plt.xlabel('label')\n",
    "plt.ylabel('count')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR/'class_distribution.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a029d1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Text length analysis\n",
    "eda_df['char_len'] = eda_df['text'].astype(str).apply(len)\n",
    "eda_df['token_list'] = eda_df['text'].astype(str).apply(word_tokenize)\n",
    "eda_df['token_len'] = eda_df['token_list'].apply(len)\n",
    "\n",
    "eda_df[['char_len','token_len']].describe().to_csv(OUT_DIR/'text_length_summary.csv')\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.subplot(1,2,1)\n",
    "sns.histplot(eda_df['char_len'], bins=50, kde=True)\n",
    "plt.title('Character length')\n",
    "plt.subplot(1,2,2)\n",
    "sns.histplot(eda_df['token_len'], bins=50, kde=True)\n",
    "plt.title('Token length')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR/'text_length_histograms.png')\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.boxplot(x='label', y='token_len', data=eda_df)\n",
    "plt.title('Token length by class')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR/'token_length_by_class.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7694350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. LIWC distributions\n",
    "liwc_cols = [c for c in eda_df.columns if c.startswith('lex_liwc_')]\n",
    "interesting = [c for c in ['lex_liwc_posemo','lex_liwc_negemo','lex_liwc_anx','lex_liwc_anger','lex_liwc_sad'] if c in liwc_cols]\n",
    "\n",
    "for c in interesting:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.histplot(eda_df[c].dropna(), bins=40, kde=True)\n",
    "    plt.title(f'Distribution: {c}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUT_DIR/f'{c}_dist.png')\n",
    "    plt.close()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sub = eda_df.melt(id_vars=['label'], value_vars=interesting, var_name='liwc', value_name='value')\n",
    "sns.violinplot(x='liwc', y='value', hue='label', data=sub, split=True)\n",
    "plt.title('Selected LIWC features by class')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR/'liwc_by_class_violin.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535a0151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. DAL distribution\n",
    "dal_cols = [c for c in eda_df.columns if c.startswith('lex_dal_')]\n",
    "if dal_cols:\n",
    "    plt.figure(figsize=(12,4))\n",
    "    for i,c in enumerate(dal_cols):\n",
    "        plt.subplot(1,len(dal_cols),i+1)\n",
    "        sns.histplot(eda_df[c].dropna(), bins=40, kde=True)\n",
    "        plt.title(c)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUT_DIR/'dal_distributions.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ae0815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Sentiment\n",
    "if 'sentiment' in eda_df.columns:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    if pd.api.types.is_numeric_dtype(eda_df['sentiment']):\n",
    "        sns.histplot(eda_df['sentiment'].dropna(), bins=40, kde=True)\n",
    "    else:\n",
    "        sns.countplot(x='sentiment', data=eda_df)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUT_DIR/'sentiment_distribution.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4516c8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Syntax complexity\n",
    "syntax_cols = [c for c in eda_df.columns if c.startswith('syntax_') or c in ['syntax_fk_grade','syntax_ari']]\n",
    "for c in syntax_cols:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.histplot(eda_df[c].dropna(), bins=40, kde=True)\n",
    "    plt.title(c)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUT_DIR/f'{c}_dist.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9d76de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Social metadata\n",
    "social_cols = [c for c in eda_df.columns if c.startswith('social_')]\n",
    "if social_cols:\n",
    "    eda_df[social_cols].describe().to_csv(OUT_DIR/'social_summary.csv')\n",
    "\n",
    "    plt.figure(figsize=(12,4))\n",
    "    for i,c in enumerate(social_cols):\n",
    "        plt.subplot(1,len(social_cols),i+1)\n",
    "        sns.histplot(eda_df[c].dropna(), bins=40, kde=True)\n",
    "        plt.title(c)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUT_DIR/'social_distributions.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ac6e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Correlation heatmap\n",
    "corr_cols = [c for c in eda_df.columns if (\n",
    "    c.startswith('lex_liwc_') or c.startswith('lex_dal_') or c.startswith('syntax_') or c=='sentiment' or c.startswith('social_')\n",
    ") and pd.api.types.is_numeric_dtype(eda_df[c])]\n",
    "\n",
    "corr_df = eda_df[corr_cols].dropna()\n",
    "pearson_corr = corr_df.corr(method='pearson')\n",
    "spearman_corr = corr_df.corr(method='spearman')\n",
    "\n",
    "plt.figure(figsize=(14,12))\n",
    "sns.heatmap(pearson_corr, cmap='vlag', center=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR/'pearson_correlation_heatmap.png')\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(14,12))\n",
    "sns.heatmap(spearman_corr, cmap='vlag', center=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR/'spearman_correlation_heatmap.png')\n",
    "plt.close()\n",
    "\n",
    "pearson_corr.to_csv(OUT_DIR/'pearson_corr.csv')\n",
    "spearman_corr.to_csv(OUT_DIR/'spearman_corr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa20007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Sample posts\n",
    "sample_by_class = eda_df.groupby('label')['text'].apply(lambda s: s.sample(n=min(10,len(s)), random_state=42)).reset_index()\n",
    "sample_by_class.to_csv(OUT_DIR/'sample_posts_by_class.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afd6585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Noise & outliers\n",
    "if 'confidence' in eda_df.columns:\n",
    "    low_conf = eda_df[eda_df['confidence'] < 0.6]\n",
    "    low_conf.to_csv(OUT_DIR/'low_confidence_labels.csv', index=False)\n",
    "\n",
    "q1 = eda_df['token_len'].quantile(0.25)\n",
    "q3 = eda_df['token_len'].quantile(0.75)\n",
    "irq = q3 - q1\n",
    "outliers_len = eda_df[(eda_df['token_len'] < (q1 - 1.5*irq)) | (eda_df['token_len'] > (q3 + 1.5*irq))]\n",
    "outliers_len.to_csv(OUT_DIR/'outliers_by_length.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9732a60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. Numeric matrix\n",
    "numeric_cols = corr_cols + ['token_len','char_len']\n",
    "X_numeric = eda_df[numeric_cols].copy()\n",
    "X_numeric.to_csv(OUT_DIR/'numeric_matrix_for_modeling.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f55692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16. Report summary\n",
    "with open(OUT_DIR/'eda_report_summary.txt','w') as f:\n",
    "    f.write('summary file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1e9c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17. Train/Test + CV\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "target = eda_df['label']\n",
    "X = eda_df[numeric_cols].fillna(0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, target, test_size=0.2, stratify=target, random_state=42\n",
    ")\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cv_results = []\n",
    "fold = 1\n",
    "\n",
    "for train_idx, val_idx in skf.split(X_train, y_train):\n",
    "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "    pipe = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", LogisticRegression(max_iter=500))\n",
    "    ])\n",
    "\n",
    "    pipe.fit(X_tr, y_tr)\n",
    "    preds = pipe.predict(X_val)\n",
    "\n",
    "    cv_results.append(accuracy_score(y_val, preds))\n",
    "\n",
    "final_model = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"clf\", LogisticRegression(max_iter=500))\n",
    "])\n",
    "\n",
    "final_model.fit(X_train, y_train)\n",
    "preds_test = final_model.predict(X_test)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
