ğŸ§  NervSightX â€” Stress Detection from Social Media using Classical Machine Learning

A complete end-to-end ML system for detecting Stress vs Non-Stress using the Dreaddit dataset.

ğŸš€ Overview

NervSightX is a classical-ML pipeline that detects stress signals in social media text.
It combines TF-IDF, Truncated SVD, and 111 psycholinguistic features (LIWC + DAL + syntax + sentiment + social metadata) into a fused machine-learning system with:

clean preprocessing

stratified 80/20 train/test split

5-fold CV without leakage

full OOF predictions

multiple base models

stacking ensemble

statistical tests

interpretability

ablation studies

Everything is implemented with classical ML only (no deep learning).

ğŸ“‚ Project Structure
NervSightX/
â”‚
â”œâ”€â”€ dreaddit_StressAnalysis - Sheet1.csv
â”‚
â”œâ”€â”€ dreaddit_cv_raw_splits/
â”‚   â”œâ”€â”€ train_raw_with_clean_text.csv
â”‚   â”œâ”€â”€ test_frozen_raw_with_clean_text.csv
â”‚   â”œâ”€â”€ tfidf/
â”‚   â”œâ”€â”€ svd/
â”‚   â”œâ”€â”€ lexical/
â”‚   â”œâ”€â”€ fused/
â”‚   â”œâ”€â”€ selected_features/
â”‚   â””â”€â”€ folds_selected/
â”‚
â”œâ”€â”€ Machine learning/
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ gaussiannb/
â”‚       â”œâ”€â”€ logreg/
â”‚       â”œâ”€â”€ svm_fast/
â”‚       â”œâ”€â”€ dt/
â”‚       â”œâ”€â”€ rf_baseline/
â”‚       â”œâ”€â”€ rf_tuned/
â”‚       â”œâ”€â”€ rf_tuned_corrected/
â”‚       â”œâ”€â”€ lgbm/
â”‚       â””â”€â”€ lgbm_tuned_quick/
â”‚
â”œâ”€â”€ dreaddit_analysis_outputs/
â”‚   â””â”€â”€ logreg_test_preds.csv
â”‚
â””â”€â”€ README.md

ğŸ“Š I. Dataset

Dreaddit Stress Analysis Dataset

Rows: 715

Columns: 116

Target: label

1 = Stress

0 = Non-Stress

Feature Groups
Group	Count	Description
LIWC (lex_liwc_*)	93	Psycholinguistic categories
DAL (lex_dal_*)	9	Activation, imagery, pleasantness
Syntax	2	ARI, FK grade
Sentiment	1	Polarity
Social	4	Karma, timestamp
Text	1	Raw + clean text
ğŸ§¹ II. Preprocessing & Feature Engineering
âœ” 1. Text Cleaning

lowercase

remove URLs, emails, markdown, mentions

keep ? and !

normalize whitespace

output â†’ clean_text

âœ” 2. Train/Test Split

Stratified 80/20

Test set frozen

Train used for 5-fold CV only

âœ” 3. TF-IDF

1â€“2 grams

Vocabulary size: 2051

Saved TF-IDF matrices + fitted vectorizer

âœ” 4. Dimensionality Reduction

TruncatedSVD (200 components)

Explained variance â‰ˆ 65.6%

âœ” 5. Lexical Features

111 LIWC + DAL + syntax + sentiment + social features

Imputed (though no missing values)

Standardized (fit only on train)

âœ” 6. Feature Fusion

Combined:
200 SVD + 111 lexical = 311 features

âœ” 7. L1 Feature Selection

L1 Logistic Regression

Reduced 311 â†’ 34 final features

These 34 were used for all CV folds & all models

ğŸ” III. Cross-Validation Pipeline

Stratified 5-fold CV

Each fold contains:

X_train_selected.npy

X_val_selected.npy

train/val CSV with orig_index, label, clean_text

Imputer + scaler applied within each fold only

No leakage into test set

ğŸ¤– IV. Base Models (with OOF Predictions)

For each model:

âœ” trained on 5 folds
âœ” generated OOF predictions
âœ” per-fold metrics
âœ” saved pipelines and CSV outputs

Models Implemented

Gaussian Naive Bayes

Logistic Regression

Linear SVM

Decision Tree

Random Forest (baseline)

Random Forest (tuned & corrected)

LightGBM (baseline)

LightGBM (quick tuned)

Best models:

Random Forest (tuned)

Linear SVM

Logistic Regression

ğŸ§¬ V. Stacking Ensemble

Using OOF predictions from all strong base models:

Constructed OOF meta-matrix

Meta-learner options:

Logistic Regression

LightGBM

Evaluated on frozen test set

Outputs saved for reproducibility

ğŸ“ˆ VI. Evaluation & Analysis
1. Metrics

Accuracy

Macro F1

Weighted F1

ROC-AUC

PR-AUC

Precision-Recall curves

Confusion matrix (raw + normalized)

Calibration curves

Brier score

2. Confidence Intervals

Bootstrap (1000 samples) to compute 95% CI for:

F1

ROC-AUC

PR-AUC

3. Statistical Significance

McNemar test: compares paired model predictions

Wilcoxon signed-rank test: compares probability outputs

Uses:

logreg_test_preds.csv

other model test predictions

ğŸ” VII. Explainability
âœ” Logistic Regression Coefficients

Interpret strongest positive/negative predictors.

âœ” SHAP

global summary

per-feature importance

per-sample local explanations

âœ” LIME

Token-level interpretability of raw text.

âœ” TF-IDF-only LR Baseline

Parallel interpretable system for human inspection.

âŒ VIII. Error Analysis

Extracted FP / FN from test set:

For each:

clean text

true label

predicted label

probability

SHAP explanation

Categorized common failure causes:

sarcasm

long trauma posts

very short posts

ambiguous sentiment

annotation noise in Dreaddit

ğŸ§¨ IX. Ablation Studies

Ablations performed:

TF-IDF only

Lexical only

Fused features

Feature selection ON/OFF

SMOTE ON/OFF

SVD dimension = 100 / 200 / 300

Time-based train/test split simulation

ğŸ X. Final Notes

Entire system is 100% classical ML

No transformers, no deep learning

Strict prevention of data leakage

Fully modular pipeline

Suitable for academic papers, hackathons, and production demos

âœ¨ Citation

Dataset: Dreaddit Stress Analysis
Author: Ayush Singh (NervSightX)
Pipeline: Custom classical-ML architecture
